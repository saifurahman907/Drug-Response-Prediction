{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dataset\n",
    "df = pd.read_csv( 'PANCANCER_ANOVA_Sun Oct 13 11_35_35 2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(columns=['log_max_conc_tested', 'log_max_conc_tested_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna(subset=['Drug target', 'Target Pathway'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Separate features and target variable \n",
    "X = df2.drop(columns=['ic50_effect_size', 'Drug name', 'Drug ID', 'Feature Name', 'Tissue Type', 'Screening Set'])\n",
    "y = df2['ic50_effect_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Drug target', 'Target Pathway', 'n_feature_pos', 'n_feature_neg',\n",
      "       'log_ic50_mean_pos', 'log_ic50_mean_neg', 'feature_ic50_t_pval',\n",
      "       'feature_delta_mean_ic50', 'feature_pos_ic50_var',\n",
      "       'feature_neg_ic50_var', 'feature_pval', 'tissue_pval', 'msi_pval',\n",
      "       'fdr'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Drug target', 'Target Pathway', 'n_feature_pos', 'n_feature_neg',\n",
      "       'log_ic50_mean_pos', 'log_ic50_mean_neg', 'feature_ic50_t_pval',\n",
      "       'feature_delta_mean_ic50', 'feature_pos_ic50_var',\n",
      "       'feature_neg_ic50_var', 'feature_pval', 'tissue_pval', 'msi_pval',\n",
      "       'fdr'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Drug target', 'Target Pathway']\n",
    "numerical_features = ['n_feature_pos', 'n_feature_neg', 'log_ic50_mean_pos', 'log_ic50_mean_neg', 'feature_ic50_t_pval', 'feature_delta_mean_ic50', 'feature_pos_ic50_var', 'feature_neg_ic50_var', 'feature_pval', 'tissue_pval', 'msi_pval', 'fdr']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numerical_features),  # Scale numerical features\n",
    "        ('cat', OneHotEncoder(), categorical_features)  # Encode categorical features\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_feature_pos', 'n_feature_neg', 'log_ic50_mean_pos', 'log_ic50_mean_neg', 'feature_ic50_t_pval', 'feature_delta_mean_ic50', 'feature_pos_ic50_var', 'feature_neg_ic50_var', 'feature_pval', 'tissue_pval', 'msi_pval', 'fdr']\n",
      "['Drug target', 'Target Pathway']\n"
     ]
    }
   ],
   "source": [
    "print(numerical_features)\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Apply scaling and encoding\n",
    "# Pipeline: This is a class provided by Scikit-learn that allows you to chain multiple steps together into a single object. \n",
    "# The steps are performed sequentially.\n",
    "# \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor) \n",
    "])\n",
    "# fit_transform: This method is used to both fit the transformer (learn the required parameters) and then transform \n",
    "# the data based on those learned parameters.\n",
    "X_train_scaled = pipeline.fit_transform(X_train)\n",
    "# This is because you should not fit the transformer on test data (which could lead to data leakage). \n",
    "# Instead, you use the parameters that were learned from the training data during fit_transform.\n",
    "The transform method applies the scaling and encoding to X_test using the precomputed parameters:\n",
    "X_test_scaled = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 0.006167793665404644\n",
      "XGBoost r^2: 0.9947604405887469\n"
     ]
    }
   ],
   "source": [
    "#  This initializes an XGBoost Regressor. XGBoost (Extreme Gradient Boosting) is a powerful and efficient machine learning algorithm \n",
    "#commonly used for regression and classification tasks, particularly with structured/tabular data. It works by building an ensemble of decision trees \n",
    "#in a sequential manner, each tree attempting to correct the errors made by the previous ones.\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using XGBoost\n",
    "y_pred_xgb = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate MAE and r^2\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost MAE: {mae_xgb}\")\n",
    "print(f\"XGBoost r^2: {r2_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "joblib.dump(model, 'Model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the preprocessing pipeline\n",
    "joblib.dump(pipeline, 'pipeline.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
